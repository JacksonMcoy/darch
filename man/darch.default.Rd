% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/darch.R
\name{darch.default}
\alias{darch.default}
\title{Fit deep neural network.}
\usage{
\method{darch}{default}(x, y, layers = 10, ..., xValid = NULL,
  yValid = NULL, caret.preProcessParams = F, normalizeWeights = F,
  normalizeWeightsBound = 15, shuffleTrainData = T,
  generateWeightsFunction = generateWeightsGlorotUniform, rbm.batchSize = 1,
  rbm.lastLayer = 0, rbm.learnRate = 1, rbm.learnRateScale = 1,
  rbm.weightDecay = 2e-04, rbm.initialMomentum = 0.5,
  rbm.finalMomentum = 0.9, rbm.momentumRampLength = 1,
  rbm.unitFunction = sigmoidUnitRbm, rbm.updateFunction = rbmUpdate,
  rbm.errorFunction = mseError, rbm.numCD = 1, rbm.numEpochs = 0,
  rbm.consecutive = T, darch = NULL, darch.batchSize = 1,
  darch.bootstrap = F, darch.fineTuneFunction = backpropagation,
  darch.initialMomentum = 0.5, darch.finalMomentum = 0.9,
  darch.momentumRampLength = 1, darch.nesterovMomentum = T,
  darch.learnRate = 1, darch.learnRateScale = 1,
  darch.errorFunction = mseError, darch.dropout = 0,
  darch.dropout.dropConnect = F, darch.dropout.momentMatching = 0,
  darch.dropout.oneMaskPerEpoch = F, darch.dither = F,
  darch.weightDecay = 0, darch.unitFunction = sigmoidUnit,
  darch.unitFunction.maxout.poolSize = 2,
  darch.weightUpdateFunction = weightDecayWeightUpdate, darch.isClass = T,
  darch.stopErr = -Inf, darch.stopClassErr = -Inf,
  darch.stopValidErr = -Inf, darch.stopValidClassErr = -Inf,
  darch.numEpochs = 100, darch.retainData = T, darch.returnBestModel = T,
  autosave = F, autosave.location = "./darch",
  autosave.epochs = round(darch.numEpochs/20), dataSet = NULL,
  dataSetValid = NULL, gputools = T, gputools.deviceId = 0,
  paramsList = list(), logLevel = NULL)
}
\arguments{
\item{x}{Input data.}

\item{y}{Target data.}

\item{layers}{Vector containing one integer for the number of neurons of each
layer. Defaults to c(\code{a}, 10, \code{b}), where \code{a} is the number
of columns in the training data and \code{b} the number of columns in the
targets. If this has length 1, it is used as the number of neurons in the
hidden layer, not as the number of layers!}

\item{...}{additional parameters}

\item{xValid}{Validation input data.}

\item{yValid}{Validation target data.}

\item{caret.preProcessParams}{List of parameters to pass to the
\code{\link{preProcess}} function for the input data or false to disable
input data pre-processing.}

\item{normalizeWeights}{Logical indicating whether to normalize weights (L2
norm = 1).}

\item{normalizeWeightsBound}{Upper bound on the L2 norm of incoming weight
vectors. Used only if \code{normalizeWeights} is \code{TRUE}.}

\item{shuffleTrainData}{Logical indicating whether to shuffle training data
before each epoch.}

\item{generateWeightsFunction}{Function to generate the initial weights of
the DBN.}

\item{rbm.batchSize}{Pre-training batch size.}

\item{rbm.lastLayer}{\code{Numeric} indicating at which layer to stop the
pre-training. Possible values include \code{0}, meaning that all layers
are trained; positive integers, meaning to stop training after the RBM
where \code{rbm.lastLayer} forms the visible layer; negative integers,
meaning to stop the training at \code{rbm.lastLayer} RBMs from the top RBM.}

\item{rbm.learnRate}{Learning rate during pre-training.}

\item{rbm.learnRateScale}{The learn rates will be multiplied with this
value after each epoch.}

\item{rbm.weightDecay}{Pre-training weight decay. Weights will be multiplied
by (1 - \code{rbm.weightDecay}) prior to each weight update.}

\item{rbm.initialMomentum}{Initial momentum during pre-training.}

\item{rbm.finalMomentum}{Final momentum during pre-training.}

\item{rbm.momentumRampLength}{After how many epochs, relative to
\code{rbm.numEpochs}, should the momentum reach \code{rbm.finalMomentum}?
A value of 1 indicates that the \code{rbm.finalMomentum} should be reached
in the final epoch, a value of 0.5 indicates that \code{rbm.finalMomentum}
should be reached after half of the training is complete.}

\item{rbm.unitFunction}{Unit function during pre-training.}

\item{rbm.updateFunction}{Update function during pre-training.}

\item{rbm.errorFunction}{Error function during pre-training.}

\item{rbm.numCD}{Number of full steps for which contrastive divergence is
performed.}

\item{rbm.numEpochs}{Number of pre-training epochs.}

\item{rbm.consecutive}{Logical indicating whether to train the RBMs one at
a time for \code{rbm.numEpochs} epochs (\code{TRUE}, default) or
alternatingly training each RBM for one epoch at a time (\code{FALSE}).}

\item{darch}{Existing \code{\linkS4class{DArch}} instance for which training
is to be resumed.}

\item{darch.batchSize}{Batch size, i.e. the number of training samples that
are presented to the network before weight updates are performed (for both
pre-training and fine-tuning).}

\item{darch.bootstrap}{Logical indicating whether to use bootstrapping to
create a training and validation data set from the given data.}

\item{darch.fineTuneFunction}{Fine-tuning function.}

\item{darch.initialMomentum}{Initial momentum during fine-tuning.}

\item{darch.finalMomentum}{Final momentum during fine-tuning.}

\item{darch.momentumRampLength}{After how many epochs, relative to
the \strong{overall} number of epochs trained, should the momentum reach
\code{darch.finalMomentum}?
A value of 1 indicates that the \code{darch.finalMomentum} should be 
reached in the final epoch, a value of 0.5 indicates that
\code{darch.finalMomentum} should be reached after half of the training is
complete. Note that this will lead to bumps in the momentum ramp if
training is resumed with the same parameters for
\code{darch.initialMomentum} and \code{darch.finalMomentum}. Set
\code{darch.momentumRampLength} to 0 to avoid this problem when resuming
training.}

\item{darch.nesterovMomentum}{Whether to use Nesterov Accelerated Momentum
(NAG) for gradient descent based fine-tuning algorithms.}

\item{darch.learnRate}{Learning rate during fine-tuning.}

\item{darch.learnRateScale}{The learning rates are multiplied by this value
after each epoch.}

\item{darch.errorFunction}{Error function during fine-tuning.}

\item{darch.dropout}{Dropout rates. If this is a vector it will be treated as
the dropout rates for each individual layer. If one element is missing, the
input dropout will be set to 0. When enabling
\code{darch.dropout.dropConnect}, this vector needs an additional element
(one element per weight matrix between two layers as opposed to one
element per layer excluding the last layer).}

\item{darch.dropout.dropConnect}{Whether to use DropConnect instead of
dropout for the hidden layers. Will use \code{darch.dropout} as the
DropConnect rates.}

\item{darch.dropout.oneMaskPerEpoch}{Whether to generate a new mask for each
batch (\code{FALSE}, default) or for each epoch (\code{TRUE}).}

\item{darch.unitFunction}{Layer function or vector of layer functions of
length \code{number of layers} - 1. Note that the first entry signifies the
layer function between layers 1 and 2, i.e. the output of layer 2. Layer 1
does not have a layer function, since the input values are used directly.}

\item{darch.unitFunction.maxout.poolSize}{Pool size for maxout units, when
using the maxout acitvation function. See \code{\link{maxoutUnit}}.}

\item{darch.weightUpdateFunction}{Weight update function or vector of weight
update functions, very similar to \code{darch.unitFunction}.}

\item{darch.isClass}{Whether output should be treated as class labels
during fine-tuning. For this, network outputs are treated as binary.}

\item{darch.stopErr}{When the value of the error function is lower than or
equal to this value, training is stopped.}

\item{darch.stopClassErr}{When the classification error is lower than or
equal to this value, training is stopped (0..100).}

\item{darch.stopValidErr}{When the value of the error function on the
validation data is lower than or equal to this value, training is stopped.}

\item{darch.stopValidClassErr}{When the classification error on the
validation data is lower than or equal to this value, training is stopped
(0..100).}

\item{darch.numEpochs}{Number of epochs of fine-tuning.}

\item{darch.retainData}{Logical indicating whether to store the training
data in the \code{\linkS4class{DArch}} instance after training.}

\item{darch.returnBestModel}{Logical indicating whether to return the best
model at the end of training, instead of the last.}

\item{autosave}{Logical indicating whether to activate automatically saving
the \code{\linkS4class{DArch}} instance to a file during fine-tuning.}

\item{autosave.location}{Path and filename of the autosave file, the file
type ".net" will be appended.}

\item{autosave.epochs}{After how many epochs should auto-saving happen, by
default after every 5% of overall progress. If this number is smaller than
1, the network will only be saved once when thee fine-tuning is done.}

\item{dataSet}{\code{\linkS4class{DataSet}} instance, passed from
darch.DataSet(), may be specified manually.}

\item{dataSetValid}{\code{\linkS4class{DataSet}} instance containing
validation data.}

\item{gputools}{Logical indicating whether to use gputools for matrix
multiplication, if available.}

\item{gputools.deviceId}{Integer specifying the device to use for GPU
matrix multiplication. See \code{\link{chooseGpu}}.}

\item{paramsList}{List of parameters, can include and does overwrite
specified parameters listed above. Primary for convenience.}

\item{logLevel}{futile.lgoger log level. Uses the currently set log level by
default, which is \code{futile.logger::flog.info} if it was not changed.
Other available levels include, from least to most verbose,
\code{FATAL}, \code{ERROR}, \code{WARN}, \code{DEBUG}, and \code{TRACE}.}

\item{darch.dropout.momentumMatching}{How many iterations to perform during
moment matching for dropout inference, 0 to disable moment matching.}
}
\value{
Fitted \code{\linkS4class{DArch}} instance
}
\description{
Fit deep neural network with optional pre-training and fine-tuning.
}
\seealso{
Other darch interface functions: \code{\link{darch.DataSet}},
  \code{\link{darch.formula}}, \code{\link{darchTest}},
  \code{\link{darch}}, \code{\link{predict.DArch}},
  \code{\link{print.DArch}}
}

