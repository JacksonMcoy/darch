% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/darchUnitFunctions.R
\name{maxoutUnit}
\alias{maxoutUnit}
\title{Maxout / LWTA unit function}
\usage{
maxoutUnit(input, ..., poolSize = getParameter(".darch.maxout.poolSize", 2,
  ...), unitFunc = getParameter(".darch.maxout.unitFunction", linearUnit,
  ...), dropoutMask = vector())
}
\arguments{
\item{input}{Input for the activation function.}

\item{...}{Additional parameters, passed on to inner unit function.}

\item{poolSize}{The size of each maxout pool.}

\item{unitFunc}{Inner unit function for maxout.}

\item{dropoutMask}{Vector containing the dropout mask.}
}
\value{
A list with the maxout activation in the first entry and the 
  derivative of the transfer function in the second entry
}
\description{
The function calculates the activation of the units and returns a list, in 
which the first entry is the result through the maxout transfer function and 
the second entry is the derivative of the transfer function.
}
\details{
Maxout sets the activations of all neurons but the one with the highest
activation within a pool to \code{0}. If this is used without
\link{maxoutWeightUpdate}, it becomes the local-winner-takes-all algorithm,
as the only difference between the two is that outgoing weights are shared
for maxout.
}
\seealso{
\linkS4class{DArch}

Other DArch unit functions: \code{\link{exponentialLinearUnit}},
  \code{\link{linearUnit}},
  \code{\link{rectifiedLinearUnit}},
  \code{\link{sigmoidUnit}}, \code{\link{softmaxUnit}},
  \code{\link{softplusUnit}}, \code{\link{tanhUnit}}
}

