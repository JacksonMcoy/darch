% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/examples.R
\name{example.xor}
\alias{example.xor}
\title{DBN XOR example.}
\description{
DBN XOR example.
}
\details{
An example using the simplest problem not solvable by a standard perceptron:
XOR.

The DBN uses three layers with 2, 2, and 1 neurons. Pre-training is generally
not as important for such small/simple problems, and it can even be
counterproductive and delay fine-tuning convergence. The learning rate is
chosen to be relatively high to achieve faster convergence. Since a sigmoid
activation function is used, higher learning rates are less problematic than,
e.g., for linear activations. Further increasing the learning rate may lead
to quicker convergence, but this effect reverses at higher values and then
leads to delayed convergence, or no convergence at all, for more complex
problems (try values higher than 10, for example). The same is true for the
momentum, which is kept at 90\% here to further increase convergence speed
(change finalMomentum to .5 to see the difference), something that is not
recommended for more complex problems.

Learning is stopped as soon as 100\% of input samples are correctly
classified.
}
\seealso{
Other darch examples: \code{\link{example.cg}};
  \code{\link{example.iris}}; \code{\link{example.maxout}};
  \code{\link{example.mnist}};
  \code{\link{example.xorNominal}}
}

